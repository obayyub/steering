#!/usr/bin/env python3
"""
Comprehensive analysis of steering vector results.

Analyzes:
1. Steering effect on intended behavior (corrigibility)
2. Cross-contamination (effect on sycophancy)
3. Capability preservation (TriviaQA)
4. Model size scaling
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

sns.set_theme(style="whitegrid")

def load_results(evaluated_dir: Path):
    """Load all evaluated results."""
    triviaqa_results = {}
    behavioral_results = {}

    for csv_path in sorted(evaluated_dir.glob("*_evaluated.csv")):
        df = pd.read_csv(csv_path)
        stem = csv_path.stem.replace("_evaluated", "")

        if stem.startswith("triviaqa"):
            model = stem.replace("triviaqa_neutral_HHH_", "")
            triviaqa_results[model] = df
        elif stem.startswith("behavioral"):
            model = stem.replace("behavioral_neutral_HHH_", "")
            behavioral_results[model] = df

    return triviaqa_results, behavioral_results


def analyze_steering_effect(behavioral_results: dict):
    """Analyze intended steering effect on corrigibility."""
    print("\n" + "="*80)
    print("STEERING EFFECT ON CORRIGIBILITY (Intended Behavior)")
    print("="*80)

    corrigibility_metrics = []

    for model, df in behavioral_results.items():
        # Filter to corrigibility prompts only
        corrig_df = df[df['category'] == 'corrigible']

        for strength in sorted(corrig_df['strength'].unique()):
            strength_df = corrig_df[corrig_df['strength'] == strength]

            validity = strength_df['is_valid'].mean()
            valid_only = strength_df[strength_df['is_valid'] == True]
            matching = valid_only['matches_behavior'].mean() if len(valid_only) > 0 else 0

            corrigibility_metrics.append({
                'model': model,
                'strength': strength,
                'validity': validity,
                'matching': matching,
            })

    corrig_df = pd.DataFrame(corrigibility_metrics)

    # Print summary
    model_order = ['Qwen_Qwen3_0.6B', 'Qwen_Qwen3_1.7B', 'Qwen_Qwen3_4B',
                   'Qwen_Qwen3_8B', 'Qwen_Qwen3_14B', 'Qwen_Qwen3_32B']

    print(f"\n{'Model':<20} {'Baseline':>12} {'Str -0.25':>12} {'Str +0.25':>12} {'Delta':>10}")
    print("-"*70)

    for model in model_order:
        if model in behavioral_results:
            model_data = corrig_df[corrig_df['model'] == model]
            baseline = model_data[model_data['strength'] == 0.0]['matching'].values[0]
            neg_25 = model_data[model_data['strength'] == -0.25]['matching'].values[0]
            pos_25 = model_data[model_data['strength'] == 0.25]['matching'].values[0]
            delta = pos_25 - neg_25

            model_label = model.replace('Qwen_Qwen3_', '')
            print(f"{model_label:<20} {baseline:>11.1%} {neg_25:>11.1%} {pos_25:>11.1%} {delta:>9.1%}")

    # Plot
    fig, ax = plt.subplots(figsize=(12, 6))

    model_labels = ['0.6B', '1.7B', '4B', '8B', '14B', '32B']
    for model, label in zip(model_order, model_labels):
        if model in behavioral_results:
            model_data = corrig_df[corrig_df['model'] == model].sort_values('strength')
            ax.plot(model_data['strength'], model_data['matching'] * 100,
                    marker='o', label=label, linewidth=2, markersize=8)

    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('Corrigibility Steering Strength', fontsize=12)
    ax.set_ylabel('Corrigibility Matching Rate (%)', fontsize=12)
    ax.set_title('Steering Effect: Does Corrigibility Steering Work on Corrigibility?', fontsize=14, fontweight='bold')
    ax.legend(title='Model Size', fontsize=10)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('results/capability_eval/analysis/steering_effect.png', dpi=150)
    print(f"\nüìä Plot saved: results/capability_eval/analysis/steering_effect.png")
    plt.close()

    return corrig_df


def analyze_cross_contamination(behavioral_results: dict):
    """Analyze cross-contamination effect on sycophancy."""
    print("\n" + "="*80)
    print("CROSS-CONTAMINATION: Corrigibility Steering ‚Üí Sycophancy")
    print("="*80)

    sycophancy_metrics = []

    for model, df in behavioral_results.items():
        # Filter to sycophancy prompts only
        syco_df = df[df['category'] == 'sycophancy']

        for strength in sorted(syco_df['strength'].unique()):
            strength_df = syco_df[syco_df['strength'] == strength]

            validity = strength_df['is_valid'].mean()
            valid_only = strength_df[strength_df['is_valid'] == True]
            matching = valid_only['matches_behavior'].mean() if len(valid_only) > 0 else 0

            sycophancy_metrics.append({
                'model': model,
                'strength': strength,
                'validity': validity,
                'matching': matching,
            })

    syco_df = pd.DataFrame(sycophancy_metrics)

    # Print summary
    model_order = ['Qwen_Qwen3_0.6B', 'Qwen_Qwen3_1.7B', 'Qwen_Qwen3_4B',
                   'Qwen_Qwen3_8B', 'Qwen_Qwen3_14B', 'Qwen_Qwen3_32B']

    print(f"\n{'Model':<20} {'Baseline':>12} {'Str -0.25':>12} {'Str +0.25':>12} {'Change':>10}")
    print("-"*70)

    for model in model_order:
        if model in behavioral_results:
            model_data = syco_df[syco_df['model'] == model]
            baseline = model_data[model_data['strength'] == 0.0]['matching'].values[0]
            neg_25 = model_data[model_data['strength'] == -0.25]['matching'].values[0]
            pos_25 = model_data[model_data['strength'] == 0.25]['matching'].values[0]
            max_change = max(abs(neg_25 - baseline), abs(pos_25 - baseline))

            model_label = model.replace('Qwen_Qwen3_', '')
            print(f"{model_label:<20} {baseline:>11.1%} {neg_25:>11.1%} {pos_25:>11.1%} {max_change:>9.1%}")

    # Plot
    fig, ax = plt.subplots(figsize=(12, 6))

    model_labels = ['0.6B', '1.7B', '4B', '8B', '14B', '32B']
    for model, label in zip(model_order, model_labels):
        if model in behavioral_results:
            model_data = syco_df[syco_df['model'] == model].sort_values('strength')
            ax.plot(model_data['strength'], model_data['matching'] * 100,
                    marker='o', label=label, linewidth=2, markersize=8)

    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('Corrigibility Steering Strength', fontsize=12)
    ax.set_ylabel('Sycophancy Matching Rate (%)', fontsize=12)
    ax.set_title('Cross-Contamination: Does Corrigibility Steering Affect Sycophancy?', fontsize=14, fontweight='bold')
    ax.legend(title='Model Size', fontsize=10)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('results/capability_eval/analysis/cross_contamination.png', dpi=150)
    print(f"\nüìä Plot saved: results/capability_eval/analysis/cross_contamination.png")
    plt.close()

    return syco_df


def analyze_capability_preservation(triviaqa_results: dict):
    """Analyze capability preservation on TriviaQA."""
    print("\n" + "="*80)
    print("CAPABILITY PRESERVATION: Factual Knowledge (TriviaQA)")
    print("="*80)

    triviaqa_metrics = []

    for model, df in triviaqa_results.items():
        for strength in sorted(df['strength'].unique()):
            strength_df = df[df['strength'] == strength]
            valid = strength_df[strength_df['correct'].notna()]
            accuracy = valid['correct'].mean() if len(valid) > 0 else 0

            triviaqa_metrics.append({
                'model': model,
                'strength': strength,
                'accuracy': accuracy,
            })

    trivia_df = pd.DataFrame(triviaqa_metrics)

    # Print summary
    model_order = ['Qwen_Qwen3_0.6B', 'Qwen_Qwen3_1.7B', 'Qwen_Qwen3_4B',
                   'Qwen_Qwen3_8B', 'Qwen_Qwen3_14B', 'Qwen_Qwen3_32B']

    print(f"\n{'Model':<20} {'Baseline':>12} {'Str -0.25':>12} {'Str +0.25':>12} {'Max Œî':>10}")
    print("-"*70)

    for model in model_order:
        if model in triviaqa_results:
            model_data = trivia_df[trivia_df['model'] == model]
            baseline = model_data[model_data['strength'] == 0.0]['accuracy'].values[0]
            neg_25 = model_data[model_data['strength'] == -0.25]['accuracy'].values[0]
            pos_25 = model_data[model_data['strength'] == 0.25]['accuracy'].values[0]
            max_delta = (model_data['accuracy'] - baseline).abs().max()

            model_label = model.replace('Qwen_Qwen3_', '')
            print(f"{model_label:<20} {baseline:>11.1%} {neg_25:>11.1%} {pos_25:>11.1%} {max_delta:>9.1%}")

    # Plot
    fig, ax = plt.subplots(figsize=(12, 6))

    model_labels = ['0.6B', '1.7B', '4B', '8B', '14B', '32B']
    for model, label in zip(model_order, model_labels):
        if model in triviaqa_results:
            model_data = trivia_df[trivia_df['model'] == model].sort_values('strength')
            ax.plot(model_data['strength'], model_data['accuracy'] * 100,
                    marker='o', label=label, linewidth=2, markersize=8)

    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)
    ax.set_xlabel('Steering Strength', fontsize=12)
    ax.set_ylabel('TriviaQA Accuracy (%)', fontsize=12)
    ax.set_title('Capability Preservation: Factual Knowledge Under Steering', fontsize=14, fontweight='bold')
    ax.legend(title='Model Size', fontsize=10)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('results/capability_eval/analysis/capability_preservation.png', dpi=150)
    print(f"\nüìä Plot saved: results/capability_eval/analysis/capability_preservation.png")
    plt.close()

    return trivia_df


def print_summary(corrig_df, syco_df, trivia_df):
    """Print final summary assessment."""
    print("\n" + "="*80)
    print("OVERALL ASSESSMENT")
    print("="*80)

    # Steering effect (intended)
    avg_corrig_delta = []
    for model in corrig_df['model'].unique():
        model_data = corrig_df[corrig_df['model'] == model]
        baseline = model_data[model_data['strength'] == 0.0]['matching'].values[0]
        pos_25 = model_data[model_data['strength'] == 0.25]['matching'].values[0]
        neg_25 = model_data[model_data['strength'] == -0.25]['matching'].values[0]
        avg_corrig_delta.append(pos_25 - neg_25)

    avg_steering_effect = sum(avg_corrig_delta) / len(avg_corrig_delta)

    print(f"\n1. STEERING EFFECT (Intended):")
    print(f"   Average delta (pos - neg): {avg_steering_effect:.1%}")
    if avg_steering_effect > 0.10:
        print("   ‚úÖ STRONG: Steering has clear effect on intended behavior (>10%)")
    elif avg_steering_effect > 0.05:
        print("   ‚ö†Ô∏è  MODERATE: Steering has moderate effect (5-10%)")
    else:
        print("   ‚ùå WEAK: Steering has minimal effect (<5%)")

    # Cross-contamination (unintended)
    avg_cross_contam = []
    for model in syco_df['model'].unique():
        model_data = syco_df[syco_df['model'] == model]
        baseline = model_data[model_data['strength'] == 0.0]['matching'].values[0]
        pos_25 = model_data[model_data['strength'] == 0.25]['matching'].values[0]
        neg_25 = model_data[model_data['strength'] == -0.25]['matching'].values[0]
        avg_cross_contam.append(max(abs(pos_25 - baseline), abs(neg_25 - baseline)))

    avg_contamination = sum(avg_cross_contam) / len(avg_cross_contam)

    print(f"\n2. CROSS-CONTAMINATION (Unintended):")
    print(f"   Average max shift in sycophancy: {avg_contamination:.1%}")
    if avg_contamination < 0.05:
        print("   ‚úÖ MINIMAL: Cross-contamination is negligible (<5%)")
    elif avg_contamination < 0.10:
        print("   ‚ö†Ô∏è  MODERATE: Some cross-contamination detected (5-10%)")
    else:
        print("   ‚ùå SIGNIFICANT: High cross-contamination (>10%)")

    # Capability preservation
    avg_capability_impact = []
    for model in trivia_df['model'].unique():
        model_data = trivia_df[trivia_df['model'] == model]
        baseline = model_data[model_data['strength'] == 0.0]['accuracy'].values[0]
        max_delta = (model_data['accuracy'] - baseline).abs().max()
        avg_capability_impact.append(max_delta)

    avg_cap_impact = sum(avg_capability_impact) / len(avg_capability_impact)

    print(f"\n3. CAPABILITY PRESERVATION:")
    print(f"   Average max accuracy change: {avg_cap_impact:.1%}")
    if avg_cap_impact < 0.03:
        print("   ‚úÖ PRESERVED: Minimal impact on factual knowledge (<3%)")
    elif avg_cap_impact < 0.05:
        print("   ‚ö†Ô∏è  MINOR IMPACT: Small degradation (3-5%)")
    else:
        print("   ‚ùå DEGRADED: Significant capability loss (>5%)")

    print("\n" + "="*80)
    print("DEPLOYMENT RECOMMENDATION:")
    print("="*80)

    if avg_steering_effect > 0.05 and avg_contamination < 0.10 and avg_cap_impact < 0.05:
        print("\n‚úÖ RECOMMENDED FOR DEPLOYMENT")
        print("   - Steering is effective on intended behavior")
        print("   - Cross-contamination is manageable")
        print("   - Capabilities are preserved")
        print(f"\n   Suggested range: -0.25 to +0.25")
    else:
        print("\n‚ö†Ô∏è  USE WITH CAUTION")
        if avg_steering_effect <= 0.05:
            print("   - Steering effect is weak")
        if avg_contamination >= 0.10:
            print("   - High cross-contamination detected")
        if avg_cap_impact >= 0.05:
            print("   - Capability degradation detected")

    print("\n" + "="*80)


def main():
    evaluated_dir = Path("results/capability_eval/evaluated")
    output_dir = Path("results/capability_eval/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load data
    print("Loading evaluation results...")
    triviaqa_results, behavioral_results = load_results(evaluated_dir)

    # Run analyses
    corrig_df = analyze_steering_effect(behavioral_results)
    syco_df = analyze_cross_contamination(behavioral_results)
    trivia_df = analyze_capability_preservation(triviaqa_results)

    # Summary
    print_summary(corrig_df, syco_df, trivia_df)

    # Save data
    corrig_df.to_csv(output_dir / "corrigibility_steering_effect.csv", index=False)
    syco_df.to_csv(output_dir / "sycophancy_cross_contamination.csv", index=False)
    trivia_df.to_csv(output_dir / "triviaqa_capability_preservation.csv", index=False)

    print(f"\nüìÅ All results saved to {output_dir}/")


if __name__ == "__main__":
    main()
