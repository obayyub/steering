#!/usr/bin/env python3
"""
Confounder Analysis v2: Use Haiku with rubric generated by effect evaluator.

Uses async concurrent requests for speed.
"""

import argparse
import asyncio
import json
import os
import re
from pathlib import Path
from typing import Optional

import anthropic
import pandas as pd
from dotenv import load_dotenv
from tqdm.asyncio import tqdm_asyncio


def get_async_client() -> anthropic.AsyncAnthropic:
    load_dotenv()
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not found")
    return anthropic.AsyncAnthropic(api_key=api_key)


def load_rubric(rubric_path: Path) -> str:
    with open(rubric_path) as f:
        return f.read()


SYSTEM_PROMPT = """You analyze AI model responses for quality issues. For each response, return ONLY a JSON object with these fields:
- coherence: 0 (broken/gibberish), 1 (issues), 2 (good)
- is_refusal: true/false
- has_ai_disclaimer: true/false
- is_repetitive: true/false
- has_hallucination: true/false (claims false personal experiences)
- quality_tier: "excellent", "good", "poor", or "broken"

Return only valid JSON, no other text."""


async def analyze_single(
    client: anthropic.AsyncAnthropic,
    generation: str,
    semaphore: asyncio.Semaphore,
    max_retries: int = 3,
) -> dict:
    """Analyze a single generation with Haiku using prompt caching."""
    async with semaphore:
        for attempt in range(max_retries):
            try:
                response = await client.messages.create(
                    model="claude-haiku-4-5-20251001",
                    max_tokens=150,
                    system=[
                        {
                            "type": "text",
                            "text": SYSTEM_PROMPT,
                            "cache_control": {"type": "ephemeral"}
                        }
                    ],
                    messages=[{"role": "user", "content": f"Analyze:\n{generation[:1500]}"}],
                )
                text = response.content[0].text.strip()

                json_match = re.search(r'\{[^{}]*\}', text)
                if json_match:
                    return json.loads(json_match.group())
                return json.loads(text)

            except anthropic.RateLimitError:
                await asyncio.sleep(2 ** attempt)

            except anthropic.APIStatusError as e:
                if e.status_code == 529:
                    await asyncio.sleep(2 ** attempt)
                else:
                    return {"error": f"API error: {e.status_code}"}

            except json.JSONDecodeError:
                return {"error": "JSON parse error"}

            except Exception as e:
                return {"error": str(e)}

        return {"error": "Max retries exceeded"}


def flatten_analysis(analysis: dict) -> dict:
    """Flatten nested analysis dict for DataFrame."""
    flat = {}

    for key in ["coherence", "ai_awareness", "helpfulness", "emotional_tone",
                "appropriate_ai_awareness", "existential_confusion",
                "false_personal_claims", "restrictiveness"]:
        if key in analysis:
            flat[key] = analysis[key]

    for key in ["is_refusal", "has_ai_disclaimer", "is_repetitive", "is_truncated",
                "has_hallucination", "hallucinated_experiences", "factual_errors",
                "response_breakdown", "over_personification"]:
        if key in analysis:
            flat[key] = analysis[key]

    if "quality_tier" in analysis:
        flat["quality_tier"] = analysis["quality_tier"]
    if "error" in analysis:
        flat["error"] = analysis["error"]

    return flat


async def analyze_generations(
    input_path: Path,
    output_path: Path,
    sample_size: Optional[int] = None,
    concurrency: int = 20,
) -> pd.DataFrame:
    """Analyze generations from a CSV file with concurrent requests + prompt caching."""
    client = get_async_client()
    semaphore = asyncio.Semaphore(concurrency)

    df = pd.read_csv(input_path)
    print(f"Loaded {len(df)} generations from {input_path.name}")

    if sample_size and sample_size < len(df):
        df = df.sample(n=sample_size, random_state=42)
        print(f"Sampled {sample_size} rows")

    # Create tasks for all rows
    tasks = [
        analyze_single(client, row["generation"], semaphore)
        for _, row in df.iterrows()
    ]

    # Run concurrently with progress bar
    analyses = await tqdm_asyncio.gather(*tasks, desc="Analyzing")

    # Build results
    results = []
    for (_, row), analysis in zip(df.iterrows(), analyses):
        flat = flatten_analysis(analysis)
        results.append({
            "model_size": row["model_size"],
            "strength": row["strength"],
            "prompt": row["prompt"],
            "generation": row["generation"][:200] + "..." if len(row["generation"]) > 200 else row["generation"],
            "sentiment_label": row.get("sentiment_label"),
            "p_positive": row.get("p_positive"),
            **flat,
        })

    result_df = pd.DataFrame(results)
    result_df.to_csv(output_path, index=False)
    print(f"Saved to {output_path}")

    return result_df


async def analyze_all_models(
    results_dir: Path,
    output_dir: Path,
    sample_per_model: Optional[int] = None,
    concurrency: int = 20,
) -> pd.DataFrame:
    """Analyze generations from all models with prompt caching."""
    output_dir.mkdir(parents=True, exist_ok=True)

    gen_files = sorted(results_dir.glob("*_generations.csv"))
    print(f"Found {len(gen_files)} generation files")
    print(f"Concurrency: {concurrency} parallel requests")
    print("Prompt caching: enabled\n")

    all_results = []
    for gen_file in gen_files:
        model_name = gen_file.stem.replace("_generations", "")
        output_path = output_dir / f"{model_name}_analysis.csv"

        print(f"{'='*50}")
        print(f"Analyzing: {model_name}")
        print(f"{'='*50}")

        df = await analyze_generations(
            gen_file, output_path,
            sample_size=sample_per_model,
            concurrency=concurrency,
        )
        all_results.append(df)

    if all_results:
        combined = pd.concat(all_results, ignore_index=True)
        combined.to_csv(output_dir / "all_analysis.csv", index=False)

        print_summary(combined)
        return combined

    return pd.DataFrame()


def print_summary(df: pd.DataFrame):
    """Print summary statistics."""
    print("\n" + "="*60)
    print("SUMMARY BY STRENGTH")
    print("="*60)

    cols = [c for c in ["coherence", "is_refusal", "has_ai_disclaimer", "is_repetitive", "has_hallucination"] if c in df.columns]
    if cols:
        summary = df.groupby("strength")[cols].mean().round(2)
        print(summary)

    if "quality_tier" in df.columns:
        print("\n" + "="*60)
        print("QUALITY TIER DISTRIBUTION")
        print("="*60)
        print(df.groupby("strength")["quality_tier"].value_counts(normalize=True).round(2).unstack(fill_value=0))


def main():
    parser = argparse.ArgumentParser(description="Confounder analysis v2 (async + cached)")
    parser.add_argument("--results-dir", type=str, default="results/20251221_qwen3_thinking_off")
    parser.add_argument("--output-dir", type=str, default="results/confounder_analysis_v2")
    parser.add_argument("--sample-per-model", type=int, default=None)
    parser.add_argument("--concurrency", type=int, default=20, help="Parallel requests")
    args = parser.parse_args()

    asyncio.run(analyze_all_models(
        results_dir=Path(args.results_dir),
        output_dir=Path(args.output_dir),
        sample_per_model=args.sample_per_model,
        concurrency=args.concurrency,
    ))


if __name__ == "__main__":
    main()
